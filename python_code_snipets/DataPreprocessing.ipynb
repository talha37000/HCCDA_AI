from sklearn.datasets import load_iris
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
import pandas as pd

# Load dataset
data = load_iris()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = data.target

# Apply SelectKBest using Chi-Squared test
selector = SelectKBest(chi2, k=2)  # Selecting top 2 features
X_new = selector.fit_transform(X, y)

# Get the selected feature columns
selected_columns = X.columns[selector.get_support()]
print(f"Selected features using Filter method: {selected_columns}")
selected_columns
from sklearn.datasets import load_iris
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
import pandas as pd

# Load dataset
data = load_iris()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = data.target

# Apply SelectKBest using Chi-Squared test
selector = SelectKBest(chi2, k='all')  # Get scores for all features
selector.fit(X, y)

# Feature scores
scores = pd.DataFrame({'Feature': X.columns, 'Score': selector.scores_})
scores = scores.sort_values(by='Score', ascending=False)

# Top features based on scores
top_features_filter = scores.head(2)  # Let's select top 2 features
print("Filter Method - Feature Scores (Chi-Squared Test):")
print(scores)
print(f"\nTop features selected: {top_features_filter['Feature'].values}")
from sklearn.datasets import load_iris, fetch_california_housing
from sklearn.feature_selection import SelectKBest, chi2, RFE
from sklearn.svm import SVC
from sklearn.linear_model import LassoCV
import pandas as pd
# 1. Load the Iris dataset for Filter and Wrapper method
iris_data = load_iris()
X_iris = pd.DataFrame(iris_data.data, columns=iris_data.feature_names)
y_iris = iris_data.target

# 2. Load California housing dataset for the Embedded method
california_data = fetch_california_housing()
X_california = pd.DataFrame(california_data.data, columns=california_data.feature_names)
y_california = california_data.target
from sklearn.feature_selection import SelectKBest, f_classif

# Apply SelectKBest using ANOVA F-test
anova_selector = SelectKBest(f_classif, k='all')
anova_selector.fit(X_iris, y_iris)

anova_scores = pd.DataFrame({'Feature': X_iris.columns, 'Score': anova_selector.scores_})
anova_scores = anova_scores.sort_values(by='Score', ascending=False)
top_features_anova = anova_scores.head(2)

print("\nFilter Method - Feature Scores (ANOVA F-test):")
print(anova_scores)
print(f"\nTop features selected: {top_features_anova['Feature'].values}")
from sklearn.feature_selection import SelectKBest, mutual_info_classif

# Apply SelectKBest using Mutual Information
mi_selector = SelectKBest(mutual_info_classif, k='all')
mi_selector.fit(X_iris, y_iris)

mi_scores = pd.DataFrame({'Feature': X_iris.columns, 'Score': mi_selector.scores_})
mi_scores = mi_scores.sort_values(by='Score', ascending=False)
top_features_mi = mi_scores.head(2)

print("\nFilter Method - Feature Scores (Mutual Information):")
print(mi_scores)
print(f"\nTop features selected: {top_features_mi['Feature'].values}")
from sklearn.feature_selection import VarianceThreshold

# Apply VarianceThreshold (removes features with low variance)
variance_selector = VarianceThreshold(threshold=0.1)  # Example threshold
variance_selector.fit(X_iris)

variance_selected = pd.DataFrame({'Feature': X_iris.columns, 'Variance': variance_selector.variances_})
variance_selected = variance_selected.sort_values(by='Variance', ascending=False)
top_features_variance = variance_selected.head(2)

print("\nFilter Method - Feature Scores (Variance Threshold):")
print(variance_selected)
print(f"\nTop features selected: {top_features_variance['Feature'].values}")
from sklearn.feature_selection import RFE
from sklearn.svm import SVC
from sklearn.datasets import load_iris
import pandas as pd

# Load dataset
data = load_iris()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = data.target

# Apply RFE with SVC as the estimator
estimator = SVC(kernel="linear")
selector = RFE(estimator, n_features_to_select=2, step=1)
X_new = selector.fit_transform(X, y)

# Get the selected feature columns
selected_columns = X.columns[selector.support_]
print(f"Selected features using Wrapper method: {selected_columns}")
# Print Scores as well

from sklearn.feature_selection import RFE
from sklearn.svm import SVC

# Apply RFE with SVC as the estimator
estimator = SVC(kernel="linear")
selector_rfe = RFE(estimator, n_features_to_select=1)  # Rank all features
selector_rfe.fit(X, y)

# Feature rankings
ranking = pd.DataFrame({'Feature': X.columns, 'Ranking': selector_rfe.ranking_})
ranking = ranking.sort_values(by='Ranking')

# Top features based on rankings
top_features_wrapper = ranking.head(2)  # Let's select top 2 features
print("\nWrapper Method - Feature Rankings (RFE with SVC):")
print(ranking)
print(f"\nTop features selected: {top_features_wrapper['Feature'].values}")

'''
In the wrapper method, Recursive Feature Elimination (RFE) ranks features based on their importance 
as evaluated by the chosen estimator (SVC in this case).
'''
from sklearn.feature_selection import RFECV

# Apply RFECV with SVC as the estimator
rfecv_selector = RFECV(estimator=SVC(kernel="linear"), step=1, cv=5)
rfecv_selector.fit(X_iris, y_iris)

rfecv_ranking = pd.DataFrame({'Feature': X_iris.columns, 'Ranking': rfecv_selector.ranking_})
rfecv_ranking = rfecv_ranking.sort_values(by='Ranking')
top_features_rfecv = rfecv_ranking.head(2)

print("\nWrapper Method - Feature Rankings (RFECV with SVC):")
print(rfecv_ranking)
print(f"\nTop features selected: {top_features_rfecv['Feature'].values}")
from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.linear_model import LogisticRegression

# Apply Sequential Feature Selection with Logistic Regression
sfs_selector = SequentialFeatureSelector(LogisticRegression(), n_features_to_select=2, direction='forward')
sfs_selector.fit(X_iris, y_iris)

sfs_ranking = pd.DataFrame({'Feature': X_iris.columns, 'Ranking': sfs_selector.get_support()})
sfs_ranking = sfs_ranking.sort_values(by='Ranking', ascending=False)
top_features_sfs = sfs_ranking.head(2)

print("\nWrapper Method - Feature Rankings (Sequential Feature Selection):")
print(sfs_ranking)
print(f"\nTop features selected: {top_features_sfs['Feature'].values}")
from sklearn.linear_model import LassoCV
from sklearn.datasets import fetch_california_housing
import pandas as pd

# Load the California housing dataset
data = fetch_california_housing()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = data.target

# Apply LassoCV (Lasso with cross-validation)
lasso = LassoCV(cv=5)
lasso.fit(X, y)

# Get the selected feature columns (non-zero coefficients)
selected_columns = X.columns[lasso.coef_ != 0]
print(f"Selected features using Embedded method (Lasso): {selected_columns}")
from sklearn.linear_model import LassoCV
from sklearn.datasets import fetch_california_housing

# Load the California housing dataset
data = fetch_california_housing()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = data.target

# Apply LassoCV (Lasso with cross-validation)
lasso = LassoCV(cv=5)
lasso.fit(X, y)

# Feature importance (coefficients)
coefficients = pd.DataFrame({'Feature': X.columns, 'Coefficient': lasso.coef_})
coefficients = coefficients.sort_values(by='Coefficient', ascending=False)

# Top features based on coefficients
top_features_embedded = coefficients.head(2)  # Let's select top 2 features
print("\nEmbedded Method - Feature Importance (Lasso):")
print(coefficients)
print(f"\nTop features selected: {top_features_embedded['Feature'].values}")
'''
In the embedded method, Lasso regression uses the coefficients as feature importance scores. 
Features with non-zero coefficients are considered important.
'''
from sklearn.tree import DecisionTreeClassifier
from sklearn.feature_selection import SelectFromModel

# Apply Decision Tree Classifier for feature importance
dt_clf = DecisionTreeClassifier(random_state=42)
dt_clf.fit(X_iris, y_iris)

dt_importance = pd.DataFrame({'Feature': X_iris.columns, 'Importance': dt_clf.feature_importances_})
dt_importance = dt_importance.sort_values(by='Importance', ascending=False)
top_features_dt = dt_importance.head(2)

print("\nEmbedded Method - Feature Importance (Decision Tree):")
print(dt_importance)
print(f"\nTop features selected: {top_features_dt['Feature'].values}")
from sklearn.ensemble import RandomForestClassifier

# Apply Random Forest Classifier for feature importance
rf_clf = RandomForestClassifier(random_state=42)
rf_clf.fit(X_iris, y_iris)

rf_importance = pd.DataFrame({'Feature': X_iris.columns, 'Importance': rf_clf.feature_importances_})
rf_importance = rf_importance.sort_values(by='Importance', ascending=False)
top_features_rf = rf_importance.head(2)

print("\nEmbedded Method - Feature Importance (Random Forest):")
print(rf_importance)
print(f"\nTop features selected: {top_features_rf['Feature'].values}")
from sklearn.ensemble import GradientBoostingClassifier

# Apply Gradient Boosting Classifier for feature importance
gb_clf = GradientBoostingClassifier(random_state=42)
gb_clf.fit(X_iris, y_iris)

gb_importance = pd.DataFrame({'Feature': X_iris.columns, 'Importance': gb_clf.feature_importances_})
gb_importance = gb_importance.sort_values(by='Importance', ascending=False)
top_features_gb = gb_importance.head(2)

print("\nEmbedded Method - Feature Importance (Gradient Boosting):")
print(gb_importance)
print(f"\nTop features selected: {top_features_gb['Feature'].values}")
