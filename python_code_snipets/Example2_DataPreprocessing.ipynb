import pandas as pd
from sklearn.datasets import load_iris, fetch_california_housing
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, VarianceThreshold, RFECV
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression, LassoCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.feature_selection import SequentialFeatureSelector
# from mlxtend.feature_selection import ExhaustiveFeatureSelector
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load datasets
iris_data = load_iris()
X_iris = pd.DataFrame(iris_data.data, columns=iris_data.feature_names)
y_iris = iris_data.target

california_data = fetch_california_housing()
X_california = pd.DataFrame(california_data.data, columns=california_data.feature_names)
y_california = california_data.target

# ------------------- 1. Filter Methods -------------------

# 1.1. ANOVA F-test
anova_selector = SelectKBest(f_classif, k='all')
anova_selector.fit(X_iris, y_iris)
anova_scores = pd.DataFrame({'Feature': X_iris.columns, 'Score': anova_selector.scores_})
anova_scores = anova_scores.sort_values(by='Score', ascending=False)

# 1.2. Mutual Information
mi_selector = SelectKBest(mutual_info_classif, k='all')
mi_selector.fit(X_iris, y_iris)
mi_scores = pd.DataFrame({'Feature': X_iris.columns, 'Score': mi_selector.scores_})
mi_scores = mi_scores.sort_values(by='Score', ascending=False)

# 1.3. Variance Threshold
variance_selector = VarianceThreshold(threshold=0.1)
variance_selector.fit(X_iris)
variance_selected = pd.DataFrame({'Feature': X_iris.columns, 'Variance': variance_selector.variances_})
variance_selected = variance_selected.sort_values(by='Variance', ascending=False)

# ------------------- 2. Wrapper Methods -------------------

# 2.1. Recursive Feature Elimination with Cross-Validation (RFECV)
rfecv_selector = RFECV(estimator=SVC(kernel="linear"), step=1, cv=5)
rfecv_selector.fit(X_iris, y_iris)
rfecv_ranking = pd.DataFrame({'Feature': X_iris.columns, 'Ranking': rfecv_selector.ranking_})
rfecv_ranking = rfecv_ranking.sort_values(by='Ranking')

# 2.2. Exhaustive Feature Selection
# efs = ExhaustiveFeatureSelector(SVC(kernel="linear"), min_features=1, max_features=3, scoring='accuracy', cv=5)
# efs.fit(X_iris, y_iris)
# efs_ranking = pd.DataFrame({'Feature': X_iris.columns, 'Rank': [i+1 for i in range(len(efs.get_support()))]})
# efs_ranking = efs_ranking.sort_values(by='Rank')

# 2.3. Sequential Feature Selection
sfs_selector = SequentialFeatureSelector(LogisticRegression(), n_features_to_select=2, direction='forward')
sfs_selector.fit(X_iris, y_iris)
sfs_ranking = pd.DataFrame({'Feature': X_iris.columns, 'Ranking': sfs_selector.get_support()})
sfs_ranking = sfs_ranking.sort_values(by='Ranking', ascending=False)

# ------------------- 3. Embedded Methods -------------------

# 3.1. Decision Tree Classifier
dt_clf = DecisionTreeClassifier(random_state=42)
dt_clf.fit(X_iris, y_iris)
dt_importance = pd.DataFrame({'Feature': X_iris.columns, 'Importance': dt_clf.feature_importances_})
dt_importance = dt_importance.sort_values(by='Importance', ascending=False)

# 3.2. Random Forest Classifier
rf_clf = RandomForestClassifier(random_state=42)
rf_clf.fit(X_iris, y_iris)
rf_importance = pd.DataFrame({'Feature': X_iris.columns, 'Importance': rf_clf.feature_importances_})
rf_importance = rf_importance.sort_values(by='Importance', ascending=False)

# 3.3. Gradient Boosting Classifier
gb_clf = GradientBoostingClassifier(random_state=42)
gb_clf.fit(X_iris, y_iris)
gb_importance = pd.DataFrame({'Feature': X_iris.columns, 'Importance': gb_clf.feature_importances_})
gb_importance = gb_importance.sort_values(by='Importance', ascending=False)

# ------------------- Train and Test AI Models -------------------

# 1. Prepare data (train/test split)
X_train, X_test, y_train, y_test = train_test_split(X_iris, y_iris, test_size=0.2, random_state=42)

# 2. Train Logistic Regression and Random Forest Classifier models

# Logistic Regression model
log_reg = LogisticRegression(max_iter=200)
log_reg.fit(X_train, y_train)
y_pred_log_reg = log_reg.predict(X_test)
log_reg_accuracy = accuracy_score(y_test, y_pred_log_reg)

# Random Forest Classifier model
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)
rf_accuracy = accuracy_score(y_test, y_pred_rf)

# ------------------- Display Results -------------------

# 1. Display Filter Methods results
print("\nFilter Method - Feature Scores (ANOVA F-test):")
print(anova_scores)
print("\nFilter Method - Feature Scores (Mutual Information):")
print(mi_scores)
print("\nFilter Method - Feature Scores (Variance Threshold):")
print(variance_selected)

# 2. Display Wrapper Methods results
print("\nWrapper Method - Feature Rankings (RFECV with SVC):")
print(rfecv_ranking)
print("\nWrapper Method - Feature Rankings (Exhaustive Feature Selection):")
# print(efs_ranking)
print("\nWrapper Method - Feature Rankings (Sequential Feature Selection):")
print(sfs_ranking)

# 3. Display Embedded Methods results
print("\nEmbedded Method - Feature Importance (Decision Tree):")
print(dt_importance)
print("\nEmbedded Method - Feature Importance (Random Forest):")
print(rf_importance)
print("\nEmbedded Method - Feature Importance (Gradient Boosting):")
print(gb_importance)

# 4. Display Model Accuracy
print(f"\nLogistic Regression Accuracy: {log_reg_accuracy:.4f}")
print(f"Random Forest Accuracy: {rf_accuracy:.4f}")

